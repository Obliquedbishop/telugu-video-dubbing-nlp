{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b><ins>Installing dependencies</ins></b>"
      ],
      "metadata": {
        "id": "WLo4bY8RmQ66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "8SOs7gBtkkFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NJn59Q5WtS_"
      },
      "outputs": [],
      "source": [
        "!pip install pytube\n",
        "!pip install git+https://github.com/openai/whisper.git -q\n",
        "!pip install pydub\n",
        "!pip install translate\n",
        "!pip install librosa\n",
        "!git clone https://github.com/x4nth055/gender-recognition-by-voice.git\n",
        "!sudo apt-get install portaudio19-dev\n",
        "!pip install pyaudio\n",
        "!pip install aksharamukha\n",
        "!pip install audio_effects\n",
        "!pip install omegaconf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.rename('gender-recognition-by-voice', 'gender_recognition_by_voice')"
      ],
      "metadata": {
        "id": "coVUHm0hqP49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28K5TBFomukd"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkzBsecyjIbJ"
      },
      "source": [
        "# <b><ins>Installing video from youtube and seperating audio and video from it</ins></b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0Xp-m2OabWl"
      },
      "outputs": [],
      "source": [
        "import pytube\n",
        "import os\n",
        "import subprocess\n",
        "from pydub import AudioSegment\n",
        "from moviepy.editor import AudioFileClip\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6uMljT5ouY2"
      },
      "outputs": [],
      "source": [
        "def convert_to_mp3(audio_obj, audio_file_path):\n",
        "  new_audio_filename = audio_obj.default_filename[:-4] + '.mp3'\n",
        "  subprocess.run([\n",
        "      'ffmpeg',\n",
        "      '-i', os.path.join(audio_file_path.strip(audio_obj.default_filename)[:-1], audio_obj.default_filename),\n",
        "      os.path.join(audio_file_path.strip(audio_obj.default_filename)[:-1], new_audio_filename)\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_audio(audio_file_path, start_time, end_time):\n",
        "    new_audio_file_path = audio_file_path.split('.')[0] + f'_clip_{start_time}_to_{end_time}'+'.' + audio_file_path.split('.')[1]\n",
        "    duration = end_time - start_time\n",
        "    ffmpeg_cmd_audio = f'ffmpeg -i \"{audio_file_path}\" -ss {start_time} -t {duration} -acodec copy \"{new_audio_file_path}\"'\n",
        "    os.system(ffmpeg_cmd_audio)\n",
        "    return new_audio_file_path"
      ],
      "metadata": {
        "id": "ugtONWUgvh9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joR7x9HMqNHJ"
      },
      "outputs": [],
      "source": [
        "def cut_video_and_audio(audio_file_path, video_file_path, start_time, end_time):\n",
        "  # Appending '_clip_{st}_to_{et} to each of audio and video to be clipped\n",
        "  new_audio_file_path = audio_file_path.split('.')[0] + f'_clip_{start_time}_to_{end_time}'+'.' + audio_file_path.split('.')[1]\n",
        "  new_video_file_path = video_file_path.split('.')[0] + f'_clip_{start_time}_to_{end_time}'+'.' + video_file_path.split('.')[1]\n",
        "\n",
        "  duration = end_time - start_time\n",
        "\n",
        "  # Build the FFmpeg command\n",
        "  ffmpeg_cmd_video = f'ffmpeg -i \"{video_file_path}\" -ss {start_time} -t {duration} -c:v copy -c:a copy \"{new_video_file_path}\"'\n",
        "  ffmpeg_cmd_audio = f'ffmpeg -i \"{audio_file_path}\" -ss {start_time} -t {duration} -acodec copy \"{new_audio_file_path}\"'\n",
        "\n",
        "  os.system(ffmpeg_cmd_video)\n",
        "  os.system(ffmpeg_cmd_audio)\n",
        "\n",
        "  \n",
        "  # removing the older audio and video files\n",
        "  os.remove(audio_file_path)\n",
        "  os.remove(video_file_path)\n",
        "\n",
        "  return new_audio_file_path, new_video_file_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmWQM2OGnW3Z"
      },
      "outputs": [],
      "source": [
        "def seperate_audio_and_video(video, clip_start_time=0, clip_end_time=60):\n",
        "  data = pytube.YouTube(video)\n",
        "  # Converting and downloading as 'MP4' file and seperating audio and video\n",
        "  audio = data.streams.get_audio_only()\n",
        "  audio_file_path = audio.download(\"audio_data\")\n",
        "  vids= data.streams\n",
        "  video_file_path = \"\"\n",
        "  for i in range(len(vids)):\n",
        "      if vids[i].resolution == '1080p' and vids[i].codecs[0] in ['vp9', 'av01.0.08M.08']:\n",
        "        print(i,'-',vids[i])\n",
        "        video_file_path = vids[i].download(\"video_data\")\n",
        "        break\n",
        "  print(\"a\",video_file_path)\n",
        "  #print(video_file_path)\n",
        "  # Creating a short clip to process on\n",
        "  audio_file_path, video_file_path = cut_video_and_audio(audio_file_path, video_file_path, clip_start_time, clip_end_time)\n",
        "\n",
        "  # Converting mp4 audio to mp3\n",
        "  convert_to_mp3(audio, audio_file_path)\n",
        "\n",
        "  return audio_file_path, video_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdGbFHBZnvNs"
      },
      "outputs": [],
      "source": [
        "# Insert the link to the youtube video you want to dub\n",
        "# video can also be clipped based on clip_start_time and clip_end_time in seconds\n",
        "# Caution: Only the videos that support the codecs 'vp9' and 'av01.0.08M.08' can be used\n",
        "original_audio_file_path, original_video_file_path = seperate_audio_and_video(\"https://www.youtube.com/live/n0f9D-vP5Pc?feature=share\", clip_start_time=1, clip_end_time=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rz_XRBQlrj3"
      },
      "source": [
        "# <b><ins>Speech Recognition model</ins></b>\n",
        "### We have used openAI's whisper model for speech recognition. It convert the hinglish audio to english text for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDcDTyutbFIN"
      },
      "outputs": [],
      "source": [
        "import whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEVwuQMTbb_2"
      },
      "outputs": [],
      "source": [
        "model = whisper.load_model('large')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK-9q9cEcBZj"
      },
      "outputs": [],
      "source": [
        "decode_options = {\"language\":\"english\"}\n",
        "text = model.transcribe(original_audio_file_path, verbose=True, word_timestamps=True, initial_prompt=\"Indian guy/lady teaching stuff\", **decode_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AXRhs0U_gCU"
      },
      "outputs": [],
      "source": [
        "text['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxoYqwC71Sic"
      },
      "outputs": [],
      "source": [
        "# To generate a word based transcript, just for testing purposes\n",
        "def generate_transcript(text):\n",
        "  for segment in text['segments']:\n",
        "    for word_dict in segment['words']:\n",
        "      if word_dict['probability'] > 0.00001:\n",
        "        print(f\"{word_dict['start']} --> {word_dict['end']} : {word_dict['word']}  {round(word_dict['probability'], 5)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1xpEd2rB14_"
      },
      "outputs": [],
      "source": [
        "generate_transcript(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj8CeUSSJOyd"
      },
      "source": [
        "# <b><ins>Machine translation</ins></b>\n",
        "### We are using google translate to sentence vise convert english text to telugu text with its corresponding timestamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkUtfM0IB5z1"
      },
      "outputs": [],
      "source": [
        "from translate import Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-EQgcEqY0zs"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "telugu_transcript = []\n",
        "for i, segment in enumerate(text['segments']):\n",
        "    translator = Translator(to_lang=\"te\")\n",
        "    result = translator.translate(segment['text'])\n",
        "    telugu_transcript.append(\n",
        "                              {\n",
        "                                \"word\":result, \n",
        "                                \"start\":segment['start'], \n",
        "                                \"end\":segment['end']\n",
        "                              }\n",
        "                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X7U675hk__N"
      },
      "outputs": [],
      "source": [
        "telugu_transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFZXDqwvn8Zm"
      },
      "outputs": [],
      "source": [
        "telugu_text = \"\"\n",
        "for sentence in telugu_transcript:\n",
        "  telugu_text += (sentence['word'] + \" \")\n",
        "telugu_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b><ins>Gender Recognition </ins></b>\n",
        "### We are now using artifical neural network with 5 dense layers from 256 units to 64 units to recognize the gender of speaker's voice."
      ],
      "metadata": {
        "id": "phqIR_gntfCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gender_recognition_by_voice.utils import load_data, split_data, create_model\n",
        "from gender_recognition_by_voice import test\n",
        "import librosa"
      ],
      "metadata": {
        "id": "EI_aWhiAvPYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()"
      ],
      "metadata": {
        "id": "chkNqYo1vVND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clipped_audio_file_path = cut_audio(original_audio_file_path, 10, 100)"
      ],
      "metadata": {
        "id": "4GAlWxwrvYes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"gender_recognition_by_voice/results/model.h5\")"
      ],
      "metadata": {
        "id": "BqV8TPB-xVsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_feature(file_name, **kwargs):\n",
        "    X, sample_rate = librosa.core.load(file_name)\n",
        "    mfcc = kwargs.get(\"mfcc\")\n",
        "    chroma = kwargs.get(\"chroma\")\n",
        "    mel = kwargs.get(\"mel\")\n",
        "    contrast = kwargs.get(\"contrast\")\n",
        "    tonnetz = kwargs.get(\"tonnetz\")\n",
        "    X, sample_rate = librosa.core.load(file_name)\n",
        "    if chroma or contrast:\n",
        "        stft = np.abs(librosa.stft(X))\n",
        "    result = np.array([])\n",
        "    if mfcc:\n",
        "        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "        result = np.hstack((result, mfccs))\n",
        "    if chroma:\n",
        "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "        result = np.hstack((result, chroma))\n",
        "    if mel:\n",
        "        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
        "        result = np.hstack((result, mel))\n",
        "    if contrast:\n",
        "        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
        "        result = np.hstack((result, contrast))\n",
        "    if tonnetz:\n",
        "        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)\n",
        "        result = np.hstack((result, tonnetz))\n",
        "    return result"
      ],
      "metadata": {
        "id": "9_ANoQpv0DLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This current model is only trained using melspectogram features. \n",
        "features_to_extract = {'mfcc':False, 'chroma':False, 'mel':True, 'contrast':False, 'tonnetz':False}\n",
        "features = extract_feature(clipped_audio_file_path, **features_to_extract).reshape(1, -1)"
      ],
      "metadata": {
        "id": "IL7zAHxsxdV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "male_prob = model.predict(features)[0][0]"
      ],
      "metadata": {
        "id": "d7SYP6NI1qRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "male_prob"
      ],
      "metadata": {
        "id": "Ch77Bd9i2iAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speaker_gender = \"\"\n",
        "if male_prob > 0.5:\n",
        "  speaker_gender = \"male\"\n",
        "else:\n",
        "  speaker_gender = \"female\""
      ],
      "metadata": {
        "id": "D7wvHfY414dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVsFMDAvpq9l"
      },
      "source": [
        "# <b><ins>Speech Synthesis model</ins></b>\n",
        "### Here too sentence-vise speech synthesis is done and finally all the sentences are concatenated together to form the output speech.\n",
        "### We first use 'aksharamukha' for transliteration, this library uses ISO romanization techniques to transliterate indic text. Then this transliterated text is fed to 'silero-tts' model for speech synthesis. Then some audio processing is done for syncing the timestamps."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_audio = AudioSegment.from_file(original_audio_file_path)"
      ],
      "metadata": {
        "id": "rfioye18fnaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from aksharamukha import transliterate\n",
        "from scipy.io import wavfile\n",
        "import torchaudio"
      ],
      "metadata": {
        "id": "MaI9VVY4owtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "akshara_output_audio = AudioSegment.empty()\n",
        "\n",
        "for i, segment in enumerate(telugu_transcript):\n",
        "  model, example_text = torch.hub.load(repo_or_dir='snakers4/silero-models', \n",
        "                                     model='silero_tts',\n",
        "                                     language='indic',\n",
        "                                     speaker='v3_indic',\n",
        "                                     verbose=False)\n",
        "  roman_text = transliterate.process('Telugu', 'ISO', segment['word'])\n",
        "  audio_path = model.save_wav(text=roman_text, speaker=f'telugu_{speaker_gender}', sample_rate=48000)\n",
        "  temp_audio_segment = AudioSegment.from_wav(audio_path)\n",
        "\n",
        "  actual_duration = (segment['end'] - segment['start'])\n",
        "\n",
        "  # If output sentence audio duration is more than actual sentence audio's duration then speed it up\n",
        "  # Else keep it as it is and add uniform pauses in the start and end of the sentence.\n",
        "  # To preserve the audio quality, speed down of audio was not performed.\n",
        "  speed = temp_audio_segment.duration_seconds/actual_duration\n",
        "  if speed > 1.01:\n",
        "    if speed-1 > 0.2:\n",
        "      speed  = 1.2\n",
        "    temp_audio_segment = temp_audio_segment.speedup(playback_speed = speed)\n",
        "  elif speed < 1:\n",
        "    sentence_pause_duration = (actual_duration - temp_audio_segment.duration_seconds)*1000\n",
        "    temp_audio_segment = AudioSegment.silent(duration=sentence_pause_duration/2) + temp_audio_segment + AudioSegment.silent(duration=sentence_pause_duration/2)\n",
        "\n",
        "  # For pauses between sentences\n",
        "  if i == 0:\n",
        "    pause_duration = segment['start'] - 0\n",
        "  else:\n",
        "    pause_duration = segment['start'] - telugu_transcript[i-1]['end']\n",
        "  \n",
        "  akshara_output_audio = akshara_output_audio + AudioSegment.silent(duration=pause_duration*1000) + temp_audio_segment\n",
        "  os.remove(audio_path)\n",
        "\n",
        "akshara_output_audio = akshara_output_audio.set_frame_rate(original_audio.frame_rate)\n",
        "akshara_output_audio = akshara_output_audio.set_channels(original_audio.channels)\n",
        "akshara_output_audio = akshara_output_audio.set_sample_width(original_audio.sample_width)\n",
        "\n",
        "# Final check to make the output audio's duration equal to the input audio\n",
        "# The above speedup of sentences still led to some minor differences betweeen actual sentence duration and output sentence duration\n",
        "final_speed = akshara_output_audio.duration_seconds/original_audio.duration_seconds\n",
        "akshara_output_audio = akshara_output_audio.speedup(playback_speed=final_speed)\n",
        "\n",
        "akshara_output_audio.export('aksharamukha_output_audio.mp3', format='mp3')\n"
      ],
      "metadata": {
        "id": "B_NPfpNSo7VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "akshara_output_audio"
      ],
      "metadata": {
        "id": "K90fHLmiprUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgWdObLuFlRF"
      },
      "outputs": [],
      "source": [
        "original_audio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b><ins>Combining Audio and Video Clips</ins></b>\n",
        "### We used 'moviepy' for combining the video with the output audio.\n",
        "### It will output the final video as 'output_video.webm'."
      ],
      "metadata": {
        "id": "0JCVT_58qxcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import *"
      ],
      "metadata": {
        "id": "VTTnQnQ2qwYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip = VideoFileClip(original_video_file_path)"
      ],
      "metadata": {
        "id": "8LHzTGf6sGqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip.duration"
      ],
      "metadata": {
        "id": "sA_cDnSFsI5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audioclip = AudioFileClip('aksharamukha_output_audio.mp3')"
      ],
      "metadata": {
        "id": "jRisnkn8sR1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if audioclip.duration > clip.duration:\n",
        "  print(f\"Audio clip is longer by {audioclip.duration - clip.duration}\")\n",
        "  audioclip = audioclip.subclip(0, clip.duration)\n",
        "else:\n",
        "  print(f\"Video clip is longer by {clip.duration - audioclip.duration}\")\n",
        "  clip = clip.subclip(0, audioclip.duration)"
      ],
      "metadata": {
        "id": "ncAPcOR6sgB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audioclip.duration, clip.duration"
      ],
      "metadata": {
        "id": "XDhTxzj7tFfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding audio to the video clip\n",
        "videoclip = clip.set_audio(audioclip)"
      ],
      "metadata": {
        "id": "i4Bxa3Lcseuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # showing video clip\n",
        "videoclip.write_videofile(\"output_video.webm\")"
      ],
      "metadata": {
        "id": "6QzD2JfGtK3m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}